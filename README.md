# ğŸš€ Telecom Analytics Real-time Pipeline

> **End-to-End Real-time Analytics Pipeline** for processing telecom events using Kafka, Spark, Airflow, and PostgreSQL.

## ğŸ“‹ Project Overview

This project implements a complete real-time analytics pipeline that:
- âœ… Generates realistic telecom events (calls, SMS, data sessions, balance recharges)
- âœ… Streams events through Apache Kafka
- âœ… Processes data in real-time using Spark Structured Streaming
- âœ… Performs daily batch aggregations
- âœ… Orchestrates workflows with Apache Airflow
- âœ… Stores results in PostgreSQL with proper indexing and partitioning

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Event     â”‚â”€â”€â”€â”€â–¶â”‚  Kafka   â”‚â”€â”€â”€â”€â–¶â”‚ Spark Streamingâ”‚â”€â”€â”€â”€â–¶â”‚ PostgreSQL   â”‚
â”‚  Generator  â”‚     â”‚  Broker  â”‚     â”‚   (Real-time)  â”‚     â”‚   Database   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  Spark Batch   â”‚
                                      â”‚  (Daily Jobs)  â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚    Airflow     â”‚
                                      â”‚ (Orchestration)â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Message Broker | Apache Kafka | 7.5.0 |
| Stream Processing | Apache Spark | 3.5.0 |
| Batch Processing | Apache Spark | 3.5.0 |
| Orchestration | Apache Airflow | 2.7.3 |
| Database | PostgreSQL | 15 |
| Containerization | Docker | 20.10+ |
| Language | Python | 3.11 |

## ğŸ“ Project Structure

```
telecom-analytics-pipeline/
â”œâ”€â”€ docker-compose.yml          # Infrastructure setup
â”œâ”€â”€ .env                        # Configuration (DO NOT COMMIT)
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ producer/               # Event generator
â”‚   â”‚   â””â”€â”€ event_generator.py
â”‚   â”œâ”€â”€ streaming/              # Spark streaming jobs
â”‚   â”‚   â””â”€â”€ spark_streaming_job.py
â”‚   â”œâ”€â”€ batch/                  # Spark batch jobs
â”‚   â”‚   â””â”€â”€ spark_batch_job.py
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ airflow/                    # Airflow configuration
â”‚   â”œâ”€â”€ dags/                   # DAG definitions
â”‚   â”‚   â””â”€â”€ daily_batch_dag.py
â”‚   â”œâ”€â”€ plugins/                # Custom plugins
â”‚   â””â”€â”€ logs/                   # Airflow logs
â”‚
â”œâ”€â”€ sql/                        # SQL scripts
â”‚   â””â”€â”€ init_db.sql             # Database initialization
â”‚
â”œâ”€â”€ data/                       # Data directories (volumes)
â”‚   â”œâ”€â”€ kafka/                  # Kafka data
â”‚   â”œâ”€â”€ postgres/               # PostgreSQL data
â”‚   â””â”€â”€ airflow/                # Airflow data
â”‚
â””â”€â”€ logs/                       # Application logs
```

## ğŸš€ Quick Start

### Prerequisites

- **Docker Desktop** installed and running
- **Python 3.9+** installed
- **16GB RAM** (recommended)
- **20GB free disk space**

### Step 1: Clone and Setup

```powershell
# Clone the repository (or create project manually)
cd C:\Users\YourName\Documents
mkdir telecom-analytics-pipeline
cd telecom-analytics-pipeline

# Copy all files from this project
```

### Step 2: Start Infrastructure

```powershell
# Make sure Docker Desktop is running

# Start all services
docker-compose up -d

# Check services status
docker-compose ps

# View logs
docker-compose logs -f
```

### Step 3: Verify Services

```powershell
# Kafka (should return empty topic list)
docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

# PostgreSQL
docker exec -it postgres psql -U telecom_user -d telecom_analytics -c "\dt"

# Airflow Web UI
# Open browser: http://localhost:8080
# Login: admin / admin123

# Spark Master UI
# Open browser: http://localhost:8081
```

### Step 4: Install Python Dependencies

```powershell
# Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Database Schema

### Tables

1. **raw_events** - All raw events from Kafka
2. **real_time_metrics** - Real-time aggregated metrics (5 & 10 min windows)
3. **anomalies** - Detected suspicious activities
4. **daily_stats** - Daily batch processing results (partitioned by date)
5. **pipeline_monitoring** - Pipeline health and execution tracking

### Views

- **v_latest_metrics** - Last 24 hours of real-time metrics
- **v_daily_summary** - Daily summary by region
- **v_active_anomalies** - Active anomalies (last 7 days)

## ğŸ¯ Pipeline Components

### 1. Event Generator (`src/producer/event_generator.py`)
- Generates realistic telecom events
- Sends to Kafka topic `telecom_events`
- Configurable event rate

### 2. Spark Streaming (`src/streaming/spark_streaming_job.py`)
- Reads from Kafka in real-time
- Performs windowed aggregations (5 and 10 minutes)
- Detects anomalies
- Writes to PostgreSQL

### 3. Spark Batch (`src/batch/spark_batch_job.py`)
- Daily processing of historical data
- Calculates daily statistics by region
- Idempotent (can be re-run safely)

### 4. Airflow DAG (`airflow/dags/daily_batch_dag.py`)
- Orchestrates daily batch job
- Runs at 2 AM daily
- Includes data validation and retry logic

## ğŸ”§ Configuration

All configuration is in `.env` file:

```bash
# Key configurations
KAFKA_BROKER=kafka:9092
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
AIRFLOW_PORT=8080
```

## ğŸ“ Development Workflow

### Running Components Locally

```powershell
# 1. Start event generator
python src/producer/event_generator.py

# 2. Submit Spark streaming job
docker exec -it spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /opt/spark-apps/streaming/spark_streaming_job.py

# 3. Run batch job manually
python src/batch/spark_batch_job.py --date 2024-11-06
```

## ğŸ› Troubleshooting

### Docker Services Not Starting

```powershell
# Check Docker is running
docker ps

# Restart services
docker-compose restart

# View logs for specific service
docker-compose logs kafka
docker-compose logs postgres
```

### Port Already in Use

```powershell
# Check which process is using the port
Get-NetTCPConnection -LocalPort 9092

# Stop the service or change port in .env
```

### Database Connection Issues

```powershell
# Test PostgreSQL connection
docker exec -it postgres psql -U telecom_user -d telecom_analytics

# Verify tables exist
\dt
```

## ğŸ“ˆ Monitoring

- **Airflow UI**: http://localhost:8080
- **Spark Master UI**: http://localhost:8081
- **PostgreSQL**: localhost:5432

## ğŸ§ª Testing

```powershell
# Run tests (once implemented)
pytest tests/

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“š Documentation

Detailed documentation for each component:

1. [Event Generator](docs/event_generator.md) - TBD
2. [Streaming Job](docs/streaming.md) - TBD
3. [Batch Job](docs/batch.md) - TBD
4. [Airflow DAG](docs/airflow.md) - TBD

## ğŸ¤ Contributing

This is a student project for KBTU. Not accepting external contributions.

## ğŸ“„ License

Educational project - All rights reserved.

## ğŸ‘¨â€ğŸ’» Author

**Arslan** - DevOps Engineer & 4th Year Student at KBTU

## ğŸ¯ Project Goals

- âœ… Infrastructure Setup (10 points)
- â³ Stream Processing (25 points)
- â³ Batch Processing (20 points)
- â³ Orchestration (30 points)
- â³ Documentation (15 points)

**Target Score: 100/100** ğŸš€

## ğŸ“… Timeline

- **Deadline**: November 9, 2025, 23:59:59
- **Status**: Infrastructure Setup Complete âœ…

---

**Need help?** Check the troubleshooting section or review the logs.